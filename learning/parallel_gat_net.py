import torch
import numpy as np
from typing import List
from torch.nn import Linear, Module, ModuleList, Sequential as LinSequential
from torch_geometric.nn import GATConv, Sequential as GeomSequential

"""
Model is designed with "row" and "columns". Each column contains the GAT model for one meta path type.
Each row contains one layer for all parallel GATs. Example for 5 meta path types:
(SG = subgraph, GAT12 = GAT layer row:1 column:2, Lin = Linear layer, out = output)

 SG0   SG1   SG2   SG3   SG4
  v     v     v     v     v 
GAT00 GAT01 GAT02 GAT03 GAT04
  v     v     v     v     v
GAT10 GAT11 GAT12 GAT13 GAT14
 \\    \\     |     //    //
  -------concatenate-------
              v
             Lin0
              v
             Lin1
              v
             Lin2
              v
             out    
"""


class ParallelGatNet(Module):

    def __init__(self, input_features_dims: List[int], feature_growth_rate: float, lin_channels: List[int], heads: int,
                 dropout: float):
        super(ParallelGatNet, self).__init__()
        # calculate  GAT layer sizes based on feature growth rate
        channels = [
            input_features_dims,
            [round(n * feature_growth_rate) for n in input_features_dims],
            [round(n * feature_growth_rate) for n in input_features_dims]
        ]
        self.input_len = len(channels[0])
        self.heads = heads
        self.gat_layers = []
        # create GAT layers
        for row_num in range(len(channels) - 1):
            row = []
            for col_num in range(len(channels[0])):
                is_not_last_row = row_num != len(channels) - 2
                # attention heads affect layer sizes
                # only use concatenation of head outputs, if layer is not in the last row, otherwise use mean
                row.append(GATConv(channels[row_num][col_num] * (heads ** row_num),
                                   channels[row_num+1][col_num] * (heads ** row_num),
                                   heads, dropout=dropout, concat=is_not_last_row))
            self.gat_layers.append(row)
        # add GAT layers to a Sequential module to simplify usage in foward()
        self.gat_nets = ModuleList()
        for col_num in range(len(channels[0])):
            self.gat_nets.append(
                GeomSequential("x, edge_index", [(layer[col_num], "x, edge_index -> x") for layer in self.gat_layers])
            )
        # create linear layers and add to a Sequential module, first layer needs to be as big as output from GAT layers
        lin_layer_dims = [sum(channels[-1]) * (heads ** (len(channels) - 2))] + lin_channels
        self.lin_net = LinSequential(*[
            Linear(lin_layer_dims[i], lin_layer_dims[i+1]) for i in range(len(lin_layer_dims) - 1)
        ])

    def forward(self, data_tuple_list: np.ndarray):
        # throw error of input does not fit model definition
        if len(data_tuple_list) != self.input_len:
            raise Exception(f"Invalid input size! Got {len(data_tuple_list)} instead of {self.input_len}.")
        # get all input vectors and edge information, process with GAT layers
        x_list = [data.x for data in data_tuple_list[:, 0]]
        edge_index_list = [data.edge_index for data in data_tuple_list[:, 0]]
        x_list = [net(x_list[col_num], edge_index_list[col_num]) for col_num, net in enumerate(self.gat_nets)]
        # get vertex IDs of root samples, those we want to classify, necessary due to graph batching
        root_idx_array = data_tuple_list[:, 1]
        root_embeddings = torch.cat([mp_x[root_indices] for mp_x, root_indices in zip(x_list, root_idx_array)], 1)
        # return output from linear layers
        return self.lin_net(root_embeddings)
