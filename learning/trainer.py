import os
import json
import time
import math
import gc
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
from typing import List

import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, ConfusionMatrixDisplay
from torch.utils.data import DataLoader, SubsetRandomSampler

from utilities.logger import Logger
from utilities.subgraph_dataset import SubgraphDataSet, list_collate
from utilities.train_val_subset_random_sampler import TrainValSubsetRandomSampler
from learning.parallel_gat_net import ParallelGatNet


@dataclass
class TrainerConfig:
    binary_classification: bool  # False: use multiclass classification
    heads_count: int
    feature_growth_rate: float
    lr: float
    weight_decay: float
    dropouts_probab: float
    data_dir: str  # path to directory containing a "processed" directory with the subgraphs (see dataset definition)
    metapath_types: List[str]  # list of all meta path types that can be found in the data directory
    max_epochs: int = 500
    verbosity: int = 4  # min=0, max=3
    seed: int = 42  # seed for torch RNG
    graph_batch_size: int = 8
    num_workers: int = 4  # number of parallel dataloader workers
    test_size: float = 0.2
    val_size: float = 0.2
    label_index_fn: str = "labels.json"  # filename of json with array of labels of all samples, expected in data_dir
    model_fn: str = f"trained_model"
    model_dir: str = os.path.join("output", "models")
    logfile_fn: str = "trained_model.log"
    log_dir: str = os.path.join("output", "logs")
    log_to_stdout: bool = True  # False: only log to file, True: log to file and stdout
    device: torch.device = "cuda" if torch.cuda.is_available() else "cpu"


class Trainer:

    def __init__(self, config: TrainerConfig):
        self.config = config
        if not os.path.exists(self.config.model_dir):
            os.makedirs(self.config.model_dir)
        self.device = torch.device(self.config.device)
        torch.manual_seed(self.config.seed)
        self.logger = Logger(self.config.log_dir, self.config.logfile_fn, self.config.verbosity,
                             log_to_stdout=self.config.log_to_stdout)
        self.logger.log("Loading datasets...", 3)
        self.dataset = SubgraphDataSet(self.config.data_dir, self.config.metapath_types)
        self.logger.log("Done.", 3)
        self.assert_batch_size()
        self.logger.log(f"Start training run with the following settings:\n{json.dumps(asdict(self.config), indent=4)}")
        self.model, self.optimizer = self.get_model_and_optim()
        self.criterion = torch.nn.BCEWithLogitsLoss()
        with open(os.path.join(config.data_dir, config.label_index_fn)) as f:
            label_index = json.load(f)
        # split data into training+validation and test set, with labels evenly distributed (stratified)
        index_train_val, self.index_test, y_train_val, self.y_test = train_test_split(
            range(len(label_index)), label_index, test_size=self.config.test_size, stratify=label_index,
            random_state=self.config.seed
        )
        # split training+validation set into training and validation set, with labels evenly distributed (stratified)
        self.index_train, self.index_val, self.y_train, self.y_val = train_test_split(
            index_train_val, y_train_val, test_size=self.config.val_size, stratify=y_train_val,
            random_state=self.config.seed
        )
        self.epoch = 0
        self.curr_model_mode = None
        self.epoch_train_losses = None
        self.epoch_val_losses = None

    def assert_batch_size(self):
        # both training and validation datasets need to be divisible by the graph batch size,
        # due to optimizations for the dataloader
        train_val_len = self.dataset.len() * (1 - self.config.test_size)
        train_len = train_val_len * (1 - self.config.val_size)
        assert train_len % self.config.graph_batch_size != 0, (
            f"Illegal batch size of {self.config.graph_batch_size} for train len {train_len}")

    def get_model_and_optim(self):
        # get input size from first element in dataset
        input_feature_dims = [data[0][1].shape[1] for data in self.dataset.get(0)]
        lin_channels = [100, 10, 1] if self.config.binary_classification else [100, 16, 8]
        # prepend a linear layer of 500 features, if >= 4 attention heads are used
        if self.config.heads_count >= 4:
            lin_channels = [500] + lin_channels
        # create model and optimizer
        model = ParallelGatNet(input_feature_dims, self.config.feature_growth_rate, lin_channels,
                               self.config.heads_count, self.config.dropouts_probab).to(self.device)
        return model, torch.optim.Adam(model.parameters(), lr=self.config.lr,
                                       weight_decay=self.config.weight_decay)

    def to_device(self, data_tuple_array: np.ndarray):
        # load data into cuda (if available)
        for data, _ in data_tuple_array:
            data.to(self.device)

    def load_model(self, epoch):
        self.logger.log(f"Loading previous model from epoch {epoch}...")
        self.model.load_state_dict(
            torch.load(os.path.join(self.config.model_dir, f"{self.config.model_fn}-{epoch:04}.pth"))
        )
        self.logger.log(f"Model from epoch {epoch} loaded.")

    def train(self):
        # calculate batch number of first validation batch, with both train and val dataset concatenated
        train_val_data_len = self.dataset.len() * (1 - self.config.test_size)
        train_data_len = train_val_data_len * (1 - self.config.val_size)
        val_idx_start = train_data_len // self.config.graph_batch_size
        sampler = TrainValSubsetRandomSampler(self.index_train, self.index_val)
        loader = DataLoader(self.dataset, sampler=sampler, collate_fn=list_collate,
                            batch_size=self.config.graph_batch_size, num_workers=self.config.num_workers)
        self.logger.log(f"Start training for max {self.config.max_epochs} epochs.")
        while self.epoch < self.config.max_epochs:
            epoch_start_time = time.time()
            # process epoch training / validation
            self.init_epoch()
            for data_batch_num, data_tuple_array in enumerate(loader):
                mode = "train" if data_batch_num < val_idx_start else "val"
                self.logger.log(f"    Working on {mode} data batch {data_batch_num+1} / {math.ceil(len(sampler) / self.config.graph_batch_size)}", 3)
                self.to_device(data_tuple_array)
                self.run(data_tuple_array, mode)
                gc.collect()
            # calculate and log losses
            train_loss = self.epoch_train_losses.mean()
            val_loss = self.epoch_val_losses.mean()
            losses = f"Train loss: {train_loss:.8f} | Val loss: {val_loss:.8f}"
            # save model for epoch
            model_fp = os.path.join(self.config.model_dir, f"{self.config.model_fn}-{self.epoch:04}.pth")
            torch.save(self.model.state_dict(), model_fp)
            epoch_time = time.time() - epoch_start_time
            self.logger.log(f"  Epoch {self.epoch:03} | {losses} | Time taken: {epoch_time:.2f}s", 2)
            self.epoch += 1

    def init_epoch(self):
        self.curr_model_mode = None
        self.epoch_train_losses = torch.tensor([]).cpu()
        self.epoch_val_losses = torch.tensor([]).cpu()

    def run(self, data_tuple_array: np.ndarray, mode: str):
        # set model to either train or eval mode, if not yet set
        if mode == "train" and self.curr_model_mode != mode:
            self.model.train()
            self.curr_model_mode = mode
        elif mode == "val" and self.curr_model_mode != mode:
            self.model.eval()
            self.curr_model_mode = mode
        # use model to get output
        out = self.model(data_tuple_array).squeeze()
        # get true labels from data
        true_labels = data_tuple_array[0][0].y[data_tuple_array[0][1]]
        true_labels = true_labels[:, 1] if self.config.binary_classification else true_labels
        if mode == "train":
            # calculate training loss and adjust weights
            loss = self.criterion(out, true_labels)
            self.epoch_train_losses = torch.cat((self.epoch_train_losses, loss.detach().unsqueeze(0).cpu()))
            loss.backward()
            self.optimizer.step()
        else:
            # calculate validation loss
            with torch.no_grad():
                self.epoch_val_losses = torch.cat((self.epoch_val_losses,
                                                   self.criterion(out, true_labels).unsqueeze(0).cpu()))

    def test(self):
        # initialize sample, dataloader and data structures
        sampler = SubsetRandomSampler(self.index_test)
        loader = DataLoader(self.dataset, sampler=sampler, collate_fn=list_collate,
                            batch_size=self.config.graph_batch_size, num_workers=self.config.num_workers)
        self.model.eval()
        self.init_epoch()
        outs = torch.tensor([]).to(self.device)
        all_true_labels = torch.tensor([]).to(self.device)
        self.logger.log("Start testing.")
        for data_batch_num, data_tuple_list in enumerate(loader):
            self.logger.log(
                f"      Testing on data batch {data_batch_num + 1} / {math.ceil(len(sampler) / self.config.graph_batch_size)}",
                4)
            self.to_device(data_tuple_list)
            # generate sigmoid(output) from model
            with torch.no_grad():
                out = self.model(data_tuple_list).sigmoid()
            # round output to 0 or 1 in case of binary classification
            # use argmax in case of multiclass classification
            out = out.round() if self.config.binary_classification else out.argmax(dim=1)
            outs = torch.cat((outs, out.squeeze()))
            # get true labels from data
            true_labels = data_tuple_list[0][0].y[data_tuple_list[0][1]]
            if self.config.binary_classification:
                true_labels = true_labels[:, 1]
            else:
                true_labels = (true_labels == 1).nonzero()[:, 1]
            all_true_labels = torch.cat((all_true_labels, true_labels))
        all_true_labels = all_true_labels.cpu()
        outs = outs.cpu()
        # calculate and log test metrics
        accuracy = accuracy_score(all_true_labels, outs)
        cf = confusion_matrix(all_true_labels, outs)
        self.logger.log(f"Test accuracy: {accuracy}")
        if self.config.binary_classification:
            precision = precision_score(all_true_labels, outs)
            self.logger.log(f"Precision:     {precision}")
        f1 = f1_score(all_true_labels.cpu(), outs.cpu(), average="micro")
        self.logger.log(f"F1-Score:      {f1}")
        self.logger.log(f"Confusion Mat:\n{cf}")
        ConfusionMatrixDisplay.from_predictions(all_true_labels, outs)
        plt.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}.png"))
