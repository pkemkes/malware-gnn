import os
import json
import time
import math
import gc
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
from typing import List
from itertools import cycle

import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
from torch.utils.data import DataLoader

from utilities.logger import Logger
from utilities.subgraph_dataset import SubgraphDataSet, list_collate
from utilities.train_val_subset_random_sampler import TrainValSubsetRandomSampler
from learning.parallel_gat_net import ParallelGatNet


@dataclass
class TrainerConfig:
    binary_classification: bool  # False: use multiclass classification
    heads_count: int
    feature_growth_rate: float
    lr: float
    weight_decay: float
    dropouts_probab: float
    data_dir: str  # path to directory containing a "processed" directory with the subgraphs (see dataset definition)
    metapath_types: List[str]  # list of all meta path types that can be found in the data directory
    max_epochs: int = 500
    verbosity: int = 4  # min=0, max=3
    seed: int = 42  # seed for torch RNG
    graph_batch_size: int = 8
    # CAUTION: don't train AND test on same dataset when using split_dataset = False, would test on training data!
    split_dataset: bool = True  # True: split dataset into train and test set, False: use full dataset for whatever task
    num_workers: int = 4  # number of parallel dataloader workers
    test_size: float = 0.2
    val_size: float = 0.2
    label_index_fn: str = "labels.json"  # filename of json with array of labels of all samples, expected in data_dir
    model_fn: str = f"trained_model"
    model_dir: str = os.path.join("output", "models")
    logfile_fn: str = "trained_model.log"
    log_dir: str = os.path.join("output", "logs")
    log_to_stdout: bool = True  # False: only log to file, True: log to file and stdout
    cmd_labels: List[str] = None
    device: torch.device = "cuda" if torch.cuda.is_available() else "cpu"


class Trainer:

    def __init__(self, config: TrainerConfig):
        self.config = config
        if not os.path.exists(self.config.model_dir):
            os.makedirs(self.config.model_dir)
        self.device = torch.device(self.config.device)
        torch.manual_seed(self.config.seed)
        self.logger = Logger(self.config.log_dir, self.config.logfile_fn, self.config.verbosity,
                             log_to_stdout=self.config.log_to_stdout)
        self.logger.log("Loading datasets...", 3)
        self.dataset = SubgraphDataSet(self.config.data_dir, self.config.metapath_types)
        self.logger.log("Done.", 3)
        self.assert_batch_size()
        self.logger.log(f"Start training run with the following settings:\n{json.dumps(asdict(self.config), indent=4)}")
        self.model, self.optimizer = self.get_model_and_optim()
        self.criterion = torch.nn.BCEWithLogitsLoss()
        with open(os.path.join(config.data_dir, config.label_index_fn)) as f:
            label_index = json.load(f)
        if self.config.split_dataset:
            # split data into train+validation and test set, with labels evenly distributed (stratified)
            index_train_val, self.index_test, y_train_val, self.y_test = train_test_split(
                range(len(label_index)), label_index, test_size=self.config.test_size, stratify=label_index,
                random_state=self.config.seed
            )
            # split train+validation set into training and validation set, with labels evenly distributed (stratified)
            self.index_train, self.index_val, self.y_train, self.y_val = train_test_split(
                index_train_val, y_train_val, test_size=self.config.val_size, stratify=y_train_val,
                random_state=self.config.seed
            )
        else:
            # split data into train and validation set, with labels evenly distributed (stratified)
            self.index_train, self.index_val, self.y_train, self.y_val = train_test_split(
                range(len(label_index)), label_index, test_size=self.config.val_size, stratify=label_index,
                random_state=self.config.seed
            )
            # and use full dataset for testing
            self.index_test = range(len(label_index))
            self.y_test = label_index
        self.epoch = 0
        self.curr_model_mode = None
        self.epoch_train_losses = None
        self.epoch_val_losses = None

    def assert_batch_size(self):
        # both training and validation datasets need to be divisible by the graph batch size,
        # due to optimizations for the dataloader
        train_val_len = self.dataset.len() * (1 - self.config.test_size)
        train_len = train_val_len * (1 - self.config.val_size)
        assert train_len % self.config.graph_batch_size == 0, (
            f"Illegal batch size of {self.config.graph_batch_size} for train len {train_len}")

    def get_model_and_optim(self):
        # get input size from first element in dataset
        input_feature_dims = [data[0][1].shape[1] for data in self.dataset.get(0)]
        lin_channels = [100, 10, 1] if self.config.binary_classification else [100, 16, 8]
        # prepend a linear layer of 500 features, if >= 4 attention heads are used
        if self.config.heads_count >= 4:
            lin_channels = [500] + lin_channels
        # create model and optimizer
        model = ParallelGatNet(input_feature_dims, self.config.feature_growth_rate, lin_channels,
                               self.config.heads_count, self.config.dropouts_probab).to(self.device)
        return model, torch.optim.Adam(model.parameters(), lr=self.config.lr,
                                       weight_decay=self.config.weight_decay)

    def to_device(self, data_tuple_array: np.ndarray):
        # load data into cuda (if available)
        for data, _ in data_tuple_array:
            data.to(self.device)

    def load_model(self, epoch):
        self.logger.log(f"Loading previous model from epoch {epoch}...")
        self.epoch = epoch + 1
        self.model.load_state_dict(
            torch.load(os.path.join(self.config.model_dir, f"{self.config.model_fn}-{epoch:04}.pth"))
        )
        self.logger.log(f"Model from epoch {epoch} loaded.")

    def train(self):
        # calculate batch number of first validation batch, with both train and val dataset concatenated
        train_val_data_len = self.dataset.len() * (1 - self.config.test_size)
        train_data_len = train_val_data_len * (1 - self.config.val_size)
        val_idx_start = train_data_len // self.config.graph_batch_size
        sampler = TrainValSubsetRandomSampler(self.index_train, self.index_val)
        loader = DataLoader(self.dataset, sampler=sampler, collate_fn=list_collate,
                            batch_size=self.config.graph_batch_size, num_workers=self.config.num_workers)
        self.logger.log(f"Start training for max {self.config.max_epochs} epochs.")
        while self.epoch < self.config.max_epochs:
            epoch_start_time = time.time()
            # process epoch training / validation
            self.init_epoch()
            for data_batch_num, data_tuple_array in enumerate(loader):
                mode = "train" if data_batch_num < val_idx_start else "val"
                self.logger.log(f"    Working on {mode} data batch {data_batch_num+1} / {math.ceil(len(sampler) / self.config.graph_batch_size)}", 3)
                self.to_device(data_tuple_array)
                self.run(data_tuple_array, mode)
                gc.collect()
            # calculate and log losses
            train_loss = self.epoch_train_losses.mean()
            val_loss = self.epoch_val_losses.mean()
            losses = f"Train loss: {train_loss:.8f} | Val loss: {val_loss:.8f}"
            # save model for epoch
            model_fp = os.path.join(self.config.model_dir, f"{self.config.model_fn}-{self.epoch:04}.pth")
            torch.save(self.model.state_dict(), model_fp)
            epoch_time = time.time() - epoch_start_time
            self.logger.log(f"  Epoch {self.epoch:03} | {losses} | Time taken: {epoch_time:.2f}s", 2)
            self.epoch += 1

    def init_epoch(self):
        self.curr_model_mode = None
        self.epoch_train_losses = torch.tensor([]).cpu()
        self.epoch_val_losses = torch.tensor([]).cpu()

    def run(self, data_tuple_array: np.ndarray, mode: str):
        # set model to either train or eval mode, if not yet set
        if mode == "train" and self.curr_model_mode != mode:
            self.model.train()
            self.curr_model_mode = mode
        elif mode == "val" and self.curr_model_mode != mode:
            self.model.eval()
            self.curr_model_mode = mode
        # use model to get output
        out = self.model(data_tuple_array).squeeze()
        # get true labels from data
        true_labels = data_tuple_array[0][0].y[data_tuple_array[0][1]]
        true_labels = true_labels[:, 1] if self.config.binary_classification else true_labels
        if mode == "train":
            # calculate training loss and adjust weights
            loss = self.criterion(out, true_labels)
            self.epoch_train_losses = torch.cat((self.epoch_train_losses, loss.detach().unsqueeze(0).cpu()))
            loss.backward()
            self.optimizer.step()
        else:
            # calculate validation loss
            with torch.no_grad():
                self.epoch_val_losses = torch.cat((self.epoch_val_losses,
                                                   self.criterion(out, true_labels).unsqueeze(0).cpu()))

    def test(self, distribution: List = None):
        # use custom label distribution if given
        if distribution is not None:
            distribution = np.array(distribution)
            # choose correct amount of target labels according to task and assert correct input
            target_length = 2 if self.config.binary_classification else 8
            assert len(distribution) == target_length, "Given distribution does not fit the amount of target labels!"
            assert distribution.sum() == 1, "Given distribution does not sum up to 1."
            # shuffle testset to enable multiple uses of this method
            shuffled_idx = torch.randperm(len(self.index_test))
            shuffled_index_test = torch.tensor(self.index_test)[shuffled_idx]
            shuffled_y_test = torch.tensor(self.y_test)[shuffled_idx]
            # calculate the number of samples per label to fit distribution
            limits_per_label = np.floor(distribution * ((len(shuffled_index_test) / target_length) / distribution.max()))
            index_test, y_test = [], []
            labels_present = [0] * target_length
            # go through all samples, keep sample if the necessary amount of samples has not yet been achieved
            for idx in range(len(shuffled_index_test)):
                label = int(shuffled_y_test[idx])
                if labels_present[label] < limits_per_label[label]:
                    index_test.append(shuffled_index_test[idx])
                    y_test.append(shuffled_y_test[idx])
                    labels_present[label] += 1
            self.logger.log(f"Using custom testset distribution: {distribution}", 2)
        else:
            index_test = self.index_test
            y_test = self.y_test
        self.logger.log(f"Testset: {[int(x) for x in index_test]}", 3)

        # initialize dataloader and data structures, sample can just be the index as no random sampling is necessary
        loader = DataLoader(self.dataset, sampler=index_test, collate_fn=list_collate,
                            batch_size=self.config.graph_batch_size, num_workers=self.config.num_workers)
        self.model.eval()
        self.init_epoch()
        outs = torch.tensor([]).to(self.device)
        all_true_labels = torch.tensor([]).to(self.device)
        all_true_label_values = torch.tensor([]).to(self.device)
        self.logger.log("Start testing.")
        for data_batch_num, data_tuple_list in enumerate(loader):
            self.logger.log(
                f"      Testing on data batch {data_batch_num + 1} / {math.ceil(len(index_test) / self.config.graph_batch_size)}",
                4)
            self.to_device(data_tuple_list)
            # generate sigmoid(output) from model
            with torch.no_grad():
                out = self.model(data_tuple_list).sigmoid()
            outs = torch.cat((outs, out.squeeze()))
            # get true labels from data
            true_labels = data_tuple_list[0][0].y[data_tuple_list[0][1]]
            if self.config.binary_classification:
                true_label_values = true_labels[:, 1]
            else:
                true_label_values = (true_labels == 1).nonzero()[:, 1]
            all_true_labels = torch.cat((all_true_labels, true_labels))
            all_true_label_values = torch.cat((all_true_label_values, true_label_values))
        all_true_labels = all_true_labels.cpu()
        all_true_label_values = all_true_label_values.cpu()
        outs = outs.cpu()
        # round output to 0 or 1 in case of binary classification
        # use argmax in case of multiclass classification
        predictions = outs.round() if self.config.binary_classification else outs.argmax(dim=1)
        # get and log misclassified samples
        misclassified_samples = torch.tensor(index_test)[all_true_label_values != predictions]
        misclassified_true = torch.tensor(y_test)[all_true_label_values != predictions]
        misclassified_pred = predictions[all_true_label_values != predictions]
        misclassified = [f"Sample: {sample}, True: {true}, Pred: {pred}"
                         for sample, true, pred in zip(misclassified_samples, misclassified_true, misclassified_pred)]
        self.logger.log(f"Misclassified samples ({len(misclassified)}): {misclassified}")
        # calculate and log test metrics
        cf = confusion_matrix(all_true_label_values, predictions)
        self.logger.log(f"Confusion Mat:\n{cf}")
        f1_micro = f1_score(all_true_label_values, predictions, average="micro")
        f1_macro = f1_score(all_true_label_values, predictions, average="macro")
        if self.config.binary_classification:
            f1_binary = f1_score(all_true_label_values, predictions)
            self.logger.log(f"F1-Score bin:   {f1_binary}")
        self.logger.log(f"F1-Score micro: {f1_micro}")
        self.logger.log(f"F1-Score macro: {f1_macro}")
        figsize = (3.5, 3.5) if self.config.binary_classification else (5, 5)
        fig, ax = plt.subplots(figsize=figsize)
        cmd = ConfusionMatrixDisplay.from_predictions(all_true_label_values, predictions,
                                                      display_labels=self.config.cmd_labels)
        cmd.plot(ax=ax)
        ax.set_xticklabels(self.config.cmd_labels, rotation=45, ha='right')
        fig.set_tight_layout(True)
        fig.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}_{int(time.time())}.png"))
        if self.config.binary_classification:
            self.plot_roc_curve(all_true_label_values, predictions, balanced=distribution is None)
            self.plot_prec_rec_curve(all_true_label_values, predictions, balanced=distribution is None)
        else:
            self.plot_roc_curve(all_true_labels, outs)
            self.plot_prec_rec_curve(all_true_labels, outs)

    """
    The following two functions are inspired by scikit-learn's examples:
    https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
    https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
    """

    def plot_roc_curve(self, true, pred, balanced: bool = True):
        lw = 2
        figsize = (4.5, 4.5) if self.config.binary_classification else (5, 5)
        fig, ax = plt.subplots(figsize=figsize)
        if self.config.binary_classification and balanced:
            fpr, tpr, _ = roc_curve(true, pred)
            roc_auc = auc(fpr, tpr)
            self.logger.log(f"ROC AUC:    {roc_auc}")
            ax.plot(
                fpr,
                tpr,
                color="darkorange",
                lw=lw,
                label=f"ROC curve (area = {roc_auc:.2f})",
            )
            ax.plot([0, 1], [0, 1], color="navy", lw=lw, linestyle="--")
        else:
            if self.config.binary_classification:
                true = torch.stack((true, 1-true)).T
                pred = torch.stack((pred, 1-pred)).T
                num_classes = 2
            else:
                num_classes = 8
            fpr, tpr, roc_auc = {}, {}, {}
            fpr["micro"], tpr["micro"], _ = roc_curve(torch.ravel(true), torch.ravel(pred))
            roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

            for n in range(num_classes):
                fpr[n], tpr[n], _ = roc_curve(true[:, n], pred[:, n])
                roc_auc[n] = auc(fpr[n], tpr[n])

            # First aggregate all false positive rates
            all_fpr = np.unique(np.concatenate([fpr[n] for n in range(num_classes)]))

            # Then interpolate all ROC curves at this points
            mean_tpr = np.zeros_like(all_fpr)
            for n in range(num_classes):
                mean_tpr += np.interp(all_fpr, fpr[n], tpr[n])

            # Finally average it and compute AUC
            mean_tpr /= num_classes

            fpr["macro"] = all_fpr
            tpr["macro"] = mean_tpr
            roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

            self.logger.log(f"ROC AUC micro:   {roc_auc['micro']}")
            self.logger.log(f"ROC AUC macro:   {roc_auc['macro']}")
            for n in range(num_classes):
                self.logger.log(f"ROC AUC class {n}: {roc_auc[n]}")

            ax.plot(fpr["micro"], tpr["micro"],
                    label=f"micro (area = {roc_auc['micro']:0.2f})",
                    color="deeppink", linestyle=":", linewidth=4)

            ax.plot(fpr["macro"], tpr["macro"],
                    label=f"macro (area = {roc_auc['macro']:0.2f})",
                    color="navy", linestyle=":", linewidth=4)

            colors = cycle(["aqua", "darkorange", "cornflowerblue"])
            for i, color in zip(range(num_classes), colors):
                ax.plot(fpr[i], tpr[i],
                        label=f"{self.config.cmd_labels[i]} (area = {roc_auc[i]:0.2f})",
                        color=color, lw=lw)

        ax.plot([0, 1], [0, 1], "k--", lw=lw, label="Worst classifier")
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel("False Positive Rate (FPR)")
        ax.set_ylabel("True Positive Rate (TPR)")
        ax.set_title("Receiver Operating Characteristic (ROC) curve")
        ax.legend(loc="lower right")
        fig.set_tight_layout(True)
        fig.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}_roc_{int(time.time())}.png"))

    def plot_prec_rec_curve(self, true, pred, balanced: bool = True):
        lw = 2
        figsize = (4.5, 4.5) if self.config.binary_classification else (5, 5)
        fig, ax = plt.subplots(figsize=figsize)
        if self.config.binary_classification and balanced:
            prec, rec, _ = precision_recall_curve(true, pred)
            ap = average_precision_score(true, pred)
            self.logger.log(f"Average Prec: {ap}")
            ax.plot(rec, prec, color="darkorange", lw=lw,
                     label=f"Precision-recall curve (AP = {ap:.2f})")
        else:
            if self.config.binary_classification:
                true = torch.stack((true, 1-true)).T
                pred = torch.stack((pred, 1-pred)).T
                num_classes = 2
            else:
                num_classes = 8
            prec, rec, ap = {}, {}, {}
            prec["micro"], rec["micro"], _ = precision_recall_curve(torch.ravel(true), torch.ravel(pred))
            ap["micro"] = average_precision_score(true, pred, average="micro")

            for n in range(num_classes):
                prec[n], rec[n], _ = precision_recall_curve(true[:, n], pred[:, n])
                ap[n] = average_precision_score(true[:, n], pred[:, n])

            # First aggregate all precisions
            all_prec = np.unique(np.concatenate([prec[n] for n in range(num_classes)]))

            # Then interpolate all prec-rec curves at this points
            mean_rec = np.zeros_like(all_prec)
            for n in range(num_classes):
                mean_rec += np.interp(all_prec, prec[n], rec[n])

            # Finally average it and compute AP
            mean_rec /= num_classes

            prec["macro"] = all_prec
            rec["macro"] = mean_rec
            ap["macro"] = average_precision_score(true, pred, average="macro")

            self.logger.log(f"Average Prec micro:   {ap['micro']}")
            self.logger.log(f"Average Prec macro:   {ap['macro']}")
            for n in range(num_classes):
                self.logger.log(f"Average Prec class {n}: {ap[n]}")

            ax.plot(rec["micro"], prec["micro"],
                    label=f"micro (AP = {ap['micro']:0.2f})",
                    color="deeppink", linestyle=":", linewidth=4)

            ax.plot(rec["macro"], prec["macro"],
                    label=f"macro (AP = {ap['macro']:0.2f})",
                    color="navy", linestyle=":", linewidth=4)

            colors = cycle(["aqua", "darkorange", "cornflowerblue"])
            for i, color in zip(range(num_classes), colors):
                ax.plot(rec[i], prec[i],
                        label=f"{self.config.cmd_labels[i]} (AP = {ap[i]:0.2f})",
                        color=color, lw=lw)

        ax.plot([0, 1], [0.5, 0.5], "k--", lw=lw, label="Worst classifier")
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel("Recall")
        ax.set_ylabel("Precision")
        ax.set_title("Precision-Recall curve")
        ax.legend(loc="lower left")
        fig.tight_layout()
        fig.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}_prec_rec_{int(time.time())}.png"))
