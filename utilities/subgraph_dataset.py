import os
import numpy as np
from typing import Union, List, Tuple, Optional, Callable

import torch
from torch_geometric.data import Dataset, Data

"""
This is a custom dataset that can work with multiple subgraph files for one sample.
It expects all data already preprocessed in the data/processed folder.
"""


class SubgraphDataSet(Dataset):

    def __init__(self, root: str, metapath_types: List[str], transform: Optional[Callable] = None,
                 pre_transform: Optional[Callable] = None):
        self.metapath_types = metapath_types
        self.in_filenames = [fn.split(".pt")[0] for fn in os.listdir(os.path.join(root, "processed"))
                             if any(mp in fn for mp in metapath_types)]
        super(SubgraphDataSet, self).__init__(root, transform, pre_transform)

    @property
    def raw_file_names(self) -> Union[str, List[str], Tuple]:
        return [f"{fn}.json" for fn in self.in_filenames]

    @property
    def processed_file_names(self) -> Union[str, List[str], Tuple]:
        return [f"{fn}.pt" for fn in self.in_filenames]

    def download(self):
        pass

    def process(self):
        raise NotImplemented

    def len(self) -> int:
        return len(self.processed_file_names) // len(self.metapath_types)

    def get(self, idx: int) -> np.ndarray:
        return np.array([self.get_single(idx, mp) for mp in self.metapath_types], dtype=object)

    def get_single(self, idx: int, mp: str) -> Data:
        return torch.load(os.path.join(self.processed_dir, f"{mp}-{idx:04}.pt"))


# graph batching function than combines all subgraphs of the same type for multiple samples
# it returns the combined graphs and the indices of the root samples that are to be classified
def list_collate(batch: List) -> np.ndarray:
    batch = np.array(batch, dtype=object)
    combined_subgraphs = [combine_subgraphs(batch[:, mp_idx]) for mp_idx in range(len(batch[0]))]
    # due to a wierd bug, numpy removes the Data structure if the batchsize is smaller than usual
    # I have no time left to fix this, therefore this dirty workaround.
    # Note the hardcoded batchsize of 8...
    if len(batch) < 8:
        return_array = np.empty((len(combined_subgraphs), 2), dtype=object)
        return_array[:] = combined_subgraphs
        return return_array
    return np.array(combined_subgraphs, dtype=object)


# combines all subgraphs in the batch, note the shifting of the edge_index according to already present nodes
def combine_subgraphs(batch: np.ndarray) -> Tuple[Data, np.ndarray]:
    all_x = torch.cat(batch[:, 0, 1].tolist())
    all_y = torch.cat(batch[:, 2, 1].tolist())

    all_edge_index = torch.cat([
        data[1][1] + sum([d[2][1].shape[0] for d in batch[:i]]) for i, data in enumerate(batch)
    ], 1)
    root_indices = np.array([sum([data[2][1].shape[0] for data in batch[:i]]) for i in range(len(batch))])
    return Data(all_x, all_edge_index, y=all_y), root_indices
