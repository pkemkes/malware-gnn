from dataclasses import dataclass
from typing import List
import json
import os
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import torch

from utilities.logger import Logger
from utilities.subgraph_dataset import SubgraphDataSet


@dataclass
class MajorityVoterConfig:
    binary_classification: bool  # False: use multiclass classification
    data_dir: str  # path to directory containing a "processed" directory with the subgraphs (see dataset definition)
    metapath_types: List[str]  # list of all meta path types that can be found in the data directory
    # multiclass_multiplicators are necessary due to the normalization of the label-features
    # a refined version should simply use a different dataset that only contains not normalized labels
    multiclass_multiplicators: List[int] = None
    seed: int = 42  # seed for torch RNG
    verbosity: int = 4  # min=0, max=4
    num_workers: int = 4
    logfile_fn: str = "majority_voter.log"
    log_dir: str = os.path.join("output", "logs")
    label_index_fn: str = "labels.json"  # filename of json with array of labels of all samples, expected in data_dir
    log_to_stdout: bool = True  # False: only log to file, True: log to file and stdout


"""
This classifies samples simply by the most common label found in the subgraphs.
If there is no distict majority, it picks at random.
"""
class MajorityVoter:

    def __init__(self, config: MajorityVoterConfig):
        self.config = config
        self.logger = Logger(self.config.log_dir, self.config.logfile_fn, self.config.verbosity,
                             log_to_stdout=self.config.log_to_stdout)
        if not self.config.binary_classification:
            assert self.config.multiclass_multiplicators is not None, (
                "Add multiclass_multiplicators for multiclass classification.")
        self.logger.log("Loading datasets...", 3)
        self.dataset = SubgraphDataSet(self.config.data_dir, self.config.metapath_types)
        self.logger.log("Done.", 3)
        with open(os.path.join(config.data_dir, config.label_index_fn)) as f:
            self.label_index = json.load(f)

    """
    Test for all samples given by the indices 
    """
    def test(self, indices: List[int]):
        # get true labels for all samples that are to be tested
        true = [self.label_index[idx] for idx in indices]
        # get predictions based on given task (binary or multiclass)
        pred = []
        for idx in indices:
            if self.config.binary_classification:
                pred.append(self.vote(idx, [2] * len(self.config.metapath_types), 2, [2], 2))
            else:
                pred.append(self.vote(idx, self.config.multiclass_multiplicators, 9, [8, 9], 8))
        # calculate and log test metrics
        accuracy = accuracy_score(true, pred)
        cf = confusion_matrix(true, pred)
        self.logger.log(f"Test accuracy: {accuracy}")
        f1 = f1_score(true, pred, average="micro")
        self.logger.log(f"F1-Score:      {f1}")
        self.logger.log(f"Confusion Mat:\n{cf}")
        ConfusionMatrixDisplay.from_predictions(true, pred)
        plt.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}.png"))

    """
    multiplicators: list of values to multpy the normalized values with to revert the normalization
    max_label: the highest value expected in the normalized labels
    unknown_to_remove: list of labels in data that are to be ignored (e.g. "unknown" or "clean")
    number_of_targets: number of target labels that can be used for classification
    """
    def vote(self, idx: int, multiplicators: List, max_label: int, unknown_to_remove: List, number_of_targets: int):
        # get sample by given index
        sample = self.dataset.get(idx)
        votes = []
        for sg_idx, subgraph in enumerate(sample):
            # for each subgraph, get the most common label
            labels = subgraph[0][1][:, 0]
            vote = self.get_most_common(labels, multiplicators[sg_idx], max_label, unknown_to_remove)
            # only add the vote, if there was any other label than "unknown"
            if vote != -1:
                votes.append(vote)
        # if all labels in the subgraphs were "unknown", not votes were added, a prediction is picked randomly
        if len(votes) == 0:
            vote = float(torch.randint(number_of_targets, (1,)))
        # else the most common vote from the subgraphs is picked
        else:
            vote = float(self.get_most_common(torch.tensor(votes), 1, 1, []))
        self.logger.log(f"  Votes for sample {idx:04}: {[float(vote) for vote in votes]} -> {vote}", 4)
        return vote

    @staticmethod
    def get_most_common(labels, multiplicator: int, max_label: int, unknown_to_remove: List):
        # revert normalization of the labels
        labels = ((labels * multiplicator) + (max_label - multiplicator))
        # get all unique labels from input
        labels_found, counts = labels.unique(return_counts=True)
        # remove all labels that are not to be used for classification
        for val_to_remove in unknown_to_remove:
            indices_of_labels_to_keep = labels_found != val_to_remove
            labels_found = labels_found[indices_of_labels_to_keep]
            counts = counts[indices_of_labels_to_keep]
        # return -1 to signal that no usable label is left
        if len(labels_found) == 0:
            return -1
        highest_count = counts.max()
        # if multiple labels are present with the same count, pick a label from this selection at random
        if (counts == highest_count).sum() != 1:
            indices_of_highest_counts = (counts == highest_count).nonzero(as_tuple=True)[0]
            ind_of_voted_label = indices_of_highest_counts[torch.randint(len(indices_of_highest_counts), (1,))]
        # else choose the most common label
        else:
            ind_of_voted_label = counts.argmax()
        return labels_found[ind_of_voted_label]
