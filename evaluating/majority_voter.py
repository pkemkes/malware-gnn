from dataclasses import dataclass
from typing import List
import json
import os
import time
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay
import torch

from utilities.logger import Logger
from utilities.subgraph_dataset import SubgraphDataSet


@dataclass
class MajorityVoterConfig:
    binary_classification: bool  # False: use multiclass classification
    data_dir: str  # path to directory containing a "processed" directory with the subgraphs (see dataset definition)
    metapath_types: List[str]  # list of all meta path types that can be found in the data directory
    # multiclass_multiplicators are necessary due to the normalization of the label-features
    # a refined version should simply use a different dataset that only contains not normalized labels
    multiclass_multiplicators: List[int] = None
    seed: int = 42  # seed for torch RNG
    verbosity: int = 4  # min=0, max=4
    num_workers: int = 4
    logfile_fn: str = "majority_voter.log"
    log_dir: str = os.path.join("output", "logs")
    label_index_fn: str = "labels.json"  # filename of json with array of labels of all samples, expected in data_dir
    log_to_stdout: bool = True  # False: only log to file, True: log to file and stdout
    cmd_labels: List[str] = None


"""
This classifies samples simply by the most common label found in the subgraphs.
If there is no distict majority, it picks at random.
"""
class MajorityVoter:

    def __init__(self, config: MajorityVoterConfig):
        self.config = config
        self.logger = Logger(self.config.log_dir, self.config.logfile_fn, self.config.verbosity,
                             log_to_stdout=self.config.log_to_stdout)
        if not self.config.binary_classification:
            assert self.config.multiclass_multiplicators is not None, (
                "Add multiclass_multiplicators for multiclass classification.")
        self.logger.log("Loading datasets...", 3)
        self.dataset = SubgraphDataSet(self.config.data_dir, self.config.metapath_types)
        self.logger.log("Done.", 3)
        with open(os.path.join(config.data_dir, config.label_index_fn)) as f:
            self.label_index = json.load(f)
        self.subgraphs_correct = None
        self.subgraphs_no_info = None
        self.subgraphs_incorrect = None

    """
    Test for all samples given by the indices 
    """
    def test(self, indices: List[int]):
        # initialize array to store performance of each subgraph and each label
        self.subgraphs_correct = np.zeros((len(self.dataset.get(indices[0])),
                                          2 if self.config.binary_classification else 8))
        self.subgraphs_no_info = np.zeros((len(self.dataset.get(indices[0])),
                                          2 if self.config.binary_classification else 8))
        self.subgraphs_incorrect = np.zeros((len(self.dataset.get(indices[0])),
                                            2 if self.config.binary_classification else 8))
        self.logger.log(f"Testset: {indices}", 4)
        # get true labels for all samples that are to be tested
        true = [self.label_index[idx] for idx in indices]
        # get predictions based on given task (binary or multiclass)
        pred = []
        for idx in indices:
            if self.config.binary_classification:
                pred.append(self.vote(idx, [2] * len(self.config.metapath_types), 2, [2], 2))
            else:
                pred.append(self.vote(idx, self.config.multiclass_multiplicators, 9, [8, 9], 8))
        # get and log all misclassified samples
        misclassified_idx = torch.tensor(true) != torch.tensor(pred)
        misclassified_samples = torch.tensor(indices)[misclassified_idx]
        misclassified_true = torch.tensor(true)[misclassified_idx]
        misclassified_pred = torch.tensor(pred)[misclassified_idx]
        misclassified = [f"Sample: {sample}, True: {true}, Pred: {pred}"
                         for sample, true, pred in zip(misclassified_samples, misclassified_true, misclassified_pred)]
        self.logger.log(f"Misclassified samples ({len(misclassified)}): {misclassified}", 3)
        # calculate and log test metrics
        cm = confusion_matrix(true, pred)
        self.logger.log(f"Confusion Mat:\n{cm}")
        f1_micro = f1_score(true, pred, average="micro")
        f1_macro = f1_score(true, pred, average="macro")
        if self.config.binary_classification:
            f1_binary = f1_score(true, pred)
            self.logger.log(f"F1-Score bin:   {f1_binary}")
        self.logger.log(f"F1-Score micro: {f1_micro}")
        self.logger.log(f"F1-Score macro: {f1_macro}")
        figsize = (3.5, 3.5) if self.config.binary_classification else (5, 5)
        fig, ax = plt.subplots(figsize=figsize)
        cmd = ConfusionMatrixDisplay.from_predictions(true, pred, display_labels=self.config.cmd_labels)
        cmd.plot(ax=ax)
        ax.set_xticklabels(self.config.cmd_labels, rotation=45, ha='right')
        fig.set_tight_layout(True)
        fig.savefig(os.path.join(self.config.log_dir, f"{self.config.logfile_fn[:-4]}_{int(time.time())}.png"))
        # log the amounts each subgraph voted for the correct label
        subgraph_performance = self.subgraphs_correct - self.subgraphs_incorrect
        self.logger.log(f"Subgraph Performance: {subgraph_performance}")
        self.logger.log(f"Subgraphs correct:   {self.subgraphs_correct}", 3)
        self.logger.log(f"Subgraphs no info:   {self.subgraphs_no_info}", 3)
        self.logger.log(f"Subgraphs incorrect: {self.subgraphs_incorrect}", 3)

    """
    multiplicators: list of values to multpy the normalized values with to revert the normalization
    max_label: the highest value expected in the normalized labels
    unknown_to_remove: list of labels in data that are to be ignored (e.g. "unknown" or "clean")
    number_of_targets: number of target labels that can be used for classification
    """
    def vote(self, idx: int, multiplicators: List, max_label: int, unknown_to_remove: List, number_of_targets: int):
        # get sample by given index
        sample = self.dataset.get(idx)
        votes = []
        for sg_idx, subgraph in enumerate(sample):
            # for each subgraph, get the most common label
            labels = subgraph[0][1][:, 0]
            vote = self.get_most_common(labels, multiplicators[sg_idx], max_label, unknown_to_remove)
            # only add the vote, if there was any other label than "unknown"
            true_label = self.label_index[idx]
            if vote != -1:
                votes.append(vote)
                # check if vote fits the correct label, increase respective performance matrix of this subgraph
                if vote == true_label:
                    self.subgraphs_correct[sg_idx, int(true_label)] += 1
                else:
                    self.subgraphs_incorrect[sg_idx, int(true_label)] += 1
                    self.logger.log(f"{self.config.metapath_types[sg_idx]}: {self.config.cmd_labels[int(true_label)]} misclassified as {self.config.cmd_labels[int(vote)]}", 4)
            else:
                self.subgraphs_no_info[sg_idx, int(true_label)] += 1
        # if all labels in the subgraphs were "unknown", not votes were added, a prediction is picked randomly
        if len(votes) == 0:
            vote = float(torch.randint(number_of_targets, (1,)))
        # else the most common vote from the subgraphs is picked
        else:
            vote = float(self.get_most_common(torch.tensor(votes), 1, 1, []))
        self.logger.log(f"  Votes for sample {idx:04}: {[float(vote) for vote in votes]} -> {vote}", 4)
        return vote

    @staticmethod
    def get_most_common(labels, multiplicator: int, max_label: int, unknown_to_remove: List):
        # revert normalization of the labels
        labels = ((labels * multiplicator) + (max_label - multiplicator))
        # get all unique labels from input
        labels_found, counts = labels.unique(return_counts=True)
        # remove all labels that are not to be used for classification
        for val_to_remove in unknown_to_remove:
            indices_of_labels_to_keep = labels_found != val_to_remove
            labels_found = labels_found[indices_of_labels_to_keep]
            counts = counts[indices_of_labels_to_keep]
        # return -1 to signal that no usable label is left
        if len(labels_found) == 0:
            return -1
        highest_count = counts.max()
        # if multiple labels are present with the same count, pick a label from this selection at random
        if (counts == highest_count).sum() != 1:
            indices_of_highest_counts = (counts == highest_count).nonzero(as_tuple=True)[0]
            ind_of_voted_label = indices_of_highest_counts[torch.randint(len(indices_of_highest_counts), (1,))]
        # else choose the most common label
        else:
            ind_of_voted_label = counts.argmax()
        return labels_found[ind_of_voted_label]
